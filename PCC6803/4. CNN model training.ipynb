{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"><span style = \"font-size:30px\"> CNN model training</span>\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "   \n",
    "    \n",
    "In this process, we built a convolutional neural network model (CNN) to predict promoter strength. We used the training data set from the previous step (see the section of ‘Acquisition of promoter dataset')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tImport python modules required for training a CNN model and import the training dataset generated in the previous step (Acquisition of promoter dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "#import kFold\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold as kf\n",
    "import theano\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "data = pd.read_csv('PCC6803 Promoter and reads 100bp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Promoter</th>\n",
       "      <th>Reads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CACCTCCAAATTATCTAAGTTAGCCAAATACCAAGAAGATTGGGCA...</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTTTGGGGAAACCGGCCGGGTCAGATTTAATCAGCGGCAACATCAC...</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCAAACGATGCTGAAGTTTACCGTTGCTGGTGAGCAGCAATAGTCC...</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAGCAACTCTTAACGGGAAATCCCAATGGTCCCTGGCAGAAAAAAT...</td>\n",
       "      <td>834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCTGAATATCTCCAGGGTTATACCGCCCCCGATGAAGCTTTTGTTT...</td>\n",
       "      <td>1496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Promoter  Reads\n",
       "0  CACCTCCAAATTATCTAAGTTAGCCAAATACCAAGAAGATTGGGCA...    106\n",
       "1  TTTTGGGGAAACCGGCCGGGTCAGATTTAATCAGCGGCAACATCAC...    643\n",
       "2  CCAAACGATGCTGAAGTTTACCGTTGCTGGTGAGCAGCAATAGTCC...    372\n",
       "3  CAGCAACTCTTAACGGGAAATCCCAATGGTCCCTGGCAGAAAAAAT...    834\n",
       "4  CCTGAATATCTCCAGGGTTATACCGCCCCCGATGAAGCTTTTGTTT...   1496"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tDefine the 'One-hot encoding' (OHE) function (see the section of ‘VAE model training’). The logarithmic scale (log2) of the reads from dRNA-seq is used as the promoter strength of each promoter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3712, 1, 4, 120)\n",
      "[[[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]]\n",
      "[[ 6.74146699]\n",
      " [ 9.33091688]\n",
      " [ 8.54303182]\n",
      " ...\n",
      " [ 5.55458885]\n",
      " [12.68145805]\n",
      " [ 9.5137276 ]]\n"
     ]
    }
   ],
   "source": [
    "# 2.\n",
    "\n",
    "def one_hot_encoding(df, seq_column, expression):\n",
    "    bases = ['A','C','G','T']\n",
    "    base_dict = dict(zip(bases,range(4)))\n",
    "    n = len(df)\n",
    "    total_width = df[seq_column].str.len().max()+20\n",
    "    X = np.zeros((n,1,4,total_width))\n",
    "    seqs = df[seq_column].values\n",
    "    for i in range(n):\n",
    "        seq = seqs[i]\n",
    "        for b in range(len(seq)):\n",
    "            X[i,0,base_dict[seq[b]], b+10+100-len(seq)] = 1    \n",
    "    X = X.astype(theano.config.floatX)\n",
    "    y = np.asarray(df[expression].values, dtype = theano.config.floatX)[:,np.newaxis]\n",
    "    return X, y, total_width\n",
    "\n",
    "Xtot, ytot, _ = one_hot_encoding(data,'Promoter','Reads')\n",
    "ytot= np.log2(ytot+1)\n",
    "print(Xtot.shape)\n",
    "print(Xtot)\n",
    "print(ytot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tDefine the 'hyperdict’. The 'hyperdict' is a variable set containing the number of kernels ('con1_num, con2_num'), the width of kernels ('con1_len', 'con2_len', 'den1_len'), the sign showing whether the computer builds the additional layer or not ('con2_prob', 'den_prob'), droplate ('droplate'), batch size ('batch_size') and epochs ('epochs')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "\n",
    "hyperdict = {'con1_num' : [4,8,16,32],'con1_len' : [6,12,18,24], 'con2_prob' : [0,1],\n",
    "             'con2_num' : [4,8,16,32],'con2_len' : [6,12,18,24],'den_prob' : [0,1],\n",
    "             'den1_len' : [4,8,16,32], 'droprate' : [0.1,0.2,0.3,0.5] , 'batch_size' :[32,64], 'epochs' : [50, 75,100, 150, 200]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tDefine the 'create_model' function. The 'create_model' function returns the model constructed with the randomly chosen elements in ‘hyperdict’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "\n",
    "def create_model(con1n = None, con1l= None,con2p = None, con2n= None, con2l= None,denp = None,den1= None, dout = None):\n",
    "    model= models.Sequential()\n",
    "    model.add(layers.Conv2D(con1n,(4, con1l), activation = 'relu', data_format = 'channels_first', input_shape = (1,4,120)))\n",
    "    model.add(layers.Dropout(dout))\n",
    "    if con2p == 1:     \n",
    "        model.add(layers.Conv2D(con2n,(1,con2l),activation = 'relu' ,data_format = 'channels_first'))\n",
    "        model.add(layers.Dropout(dout))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(dout))\n",
    "    if denp == 1:\n",
    "        model.add(layers.Dense(den1, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tTrain the CNN model. This code helps you yield the best model with a minimum loss function value. In addition, to prevent possible overfitting during the training, we conduct k-fold cross-validation (k = 5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 1s 319us/sample - loss: 19.3778 - mean_squared_error: 19.3778\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 0s 167us/sample - loss: 6.4948 - mean_squared_error: 6.4948\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 0s 154us/sample - loss: 6.4092 - mean_squared_error: 6.4092\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 0s 146us/sample - loss: 6.2410 - mean_squared_error: 6.2410\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 0s 137us/sample - loss: 6.1439 - mean_squared_error: 6.1439\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 1s 440us/sample - loss: 15.2891 - mean_squared_error: 15.2891\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 1s 267us/sample - loss: 6.5657 - mean_squared_error: 6.5657\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 1s 253us/sample - loss: 6.3260 - mean_squared_error: 6.3260\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 1s 288us/sample - loss: 6.2914 - mean_squared_error: 6.2914\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 1s 264us/sample - loss: 6.1141 - mean_squared_error: 6.1141\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 13.1281 - mean_squared_error: 13.1281\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 3s 1ms/sample - loss: 6.5492 - mean_squared_error: 6.5492\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 4s 2ms/sample - loss: 6.5080 - mean_squared_error: 6.5080\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 5s 2ms/sample - loss: 6.4050 - mean_squared_error: 6.4050\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 3s 920us/sample - loss: 6.2490 - mean_squared_error: 6.2490\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 5s 2ms/sample - loss: 14.7978 - mean_squared_error: 14.7978\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 6.9388 - mean_squared_error: 6.9388\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 6.7418 - mean_squared_error: 6.7418\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 6.5975 - mean_squared_error: 6.5975\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 6.6993 - mean_squared_error: 6.6993\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 471us/sample - loss: 21.9101 - mean_squared_error: 21.9101\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 1s 351us/sample - loss: 7.1959 - mean_squared_error: 7.1959\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 343us/sample - loss: 7.0694 - mean_squared_error: 7.0694\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 320us/sample - loss: 6.9082 - mean_squared_error: 6.9082\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 1s 333us/sample - loss: 7.0730 - mean_squared_error: 7.0730\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 5s 2ms/sample - loss: 28.6767 - mean_squared_error: 28.6767\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.9665 - mean_squared_error: 6.9665\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 7.0162 - mean_squared_error: 7.0162\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.8310 - mean_squared_error: 6.8310\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.7740 - mean_squared_error: 6.7740\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 2s 777us/sample - loss: 33.5263 - mean_squared_error: 33.5263\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 3s 993us/sample - loss: 6.6908 - mean_squared_error: 6.6908\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 3s 1ms/sample - loss: 6.5075 - mean_squared_error: 6.5075\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 3s 881us/sample - loss: 6.4662 - mean_squared_error: 6.4662\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 2s 795us/sample - loss: 6.4667 - mean_squared_error: 6.4667\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 461us/sample - loss: 24.8776 - mean_squared_error: 24.8776\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 1s 489us/sample - loss: 6.6504 - mean_squared_error: 6.6504\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 410us/sample - loss: 6.5408 - mean_squared_error: 6.5408\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 338us/sample - loss: 6.4046 - mean_squared_error: 6.4046\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 1s 467us/sample - loss: 6.3583 - mean_squared_error: 6.3583\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 2s 659us/sample - loss: 25.0006 - mean_squared_error: 25.0006\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 1s 405us/sample - loss: 6.4590 - mean_squared_error: 6.4590\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 264us/sample - loss: 6.3728 - mean_squared_error: 6.3728\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 412us/sample - loss: 6.3722 - mean_squared_error: 6.3722\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 1s 304us/sample - loss: 6.3490 - mean_squared_error: 6.3490\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 3s 959us/sample - loss: 26.3463 - mean_squared_error: 26.3463\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 2s 618us/sample - loss: 7.4124 - mean_squared_error: 7.4124\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 2s 711us/sample - loss: 7.1524 - mean_squared_error: 7.1524\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 2s 731us/sample - loss: 7.3009 - mean_squared_error: 7.3009\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 2s 778us/sample - loss: 7.1348 - mean_squared_error: 7.1348\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 5s 2ms/sample - loss: 23.0406 - mean_squared_error: 23.0406\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.8024 - mean_squared_error: 6.8024\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.6151 - mean_squared_error: 6.6151\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.4923 - mean_squared_error: 6.4923\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.5714 - mean_squared_error: 6.5714\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 1s 395us/sample - loss: 15.6918 - mean_squared_error: 15.6918\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 1s 289us/sample - loss: 7.0208 - mean_squared_error: 7.0208\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 1s 254us/sample - loss: 6.8148 - mean_squared_error: 6.8148\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 1s 297us/sample - loss: 6.6042 - mean_squared_error: 6.6042\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 1s 279us/sample - loss: 6.4519 - mean_squared_error: 6.4519\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 5s 2ms/sample - loss: 17.0502 - mean_squared_error: 17.0502\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 5s 2ms/sample - loss: 6.6324 - mean_squared_error: 6.6324\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 5s 2ms/sample - loss: 6.6353 - mean_squared_error: 6.6353\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 6.4124 - mean_squared_error: 6.4124\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 5s 2ms/sample - loss: 6.2132 - mean_squared_error: 6.2132\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 7s 2ms/sample - loss: 22.8729 - mean_squared_error: 22.8729\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 6s 2ms/sample - loss: 7.9252 - mean_squared_error: 7.9252\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 6s 2ms/sample - loss: 7.6813 - mean_squared_error: 7.6813\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 6s 2ms/sample - loss: 7.4999 - mean_squared_error: 7.4999\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 6s 2ms/sample - loss: 7.3741 - mean_squared_error: 7.3741\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 348us/sample - loss: 25.3300 - mean_squared_error: 25.3300\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 1s 192us/sample - loss: 8.5194 - mean_squared_error: 8.5194\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 253us/sample - loss: 8.1316 - mean_squared_error: 8.1316\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 184us/sample - loss: 7.8681 - mean_squared_error: 7.8681\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 1s 254us/sample - loss: 7.6224 - mean_squared_error: 7.6224\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 2s 580us/sample - loss: 21.5838 - mean_squared_error: 21.5838\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 1s 404us/sample - loss: 6.7791 - mean_squared_error: 6.7791\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 1s 469us/sample - loss: 6.5945 - mean_squared_error: 6.5945\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 1s 354us/sample - loss: 6.5273 - mean_squared_error: 6.5273\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 1s 334us/sample - loss: 6.5014 - mean_squared_error: 6.5014\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 2s 511us/sample - loss: 14.6519 - mean_squared_error: 14.6519\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 2s 599us/sample - loss: 6.4701 - mean_squared_error: 6.4701\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 1s 402us/sample - loss: 6.2092 - mean_squared_error: 6.2092\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 2s 520us/sample - loss: 6.0386 - mean_squared_error: 6.0386\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 1s 416us/sample - loss: 6.0307 - mean_squared_error: 6.0307\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 374us/sample - loss: 23.9257 - mean_squared_error: 23.9257\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 0s 157us/sample - loss: 7.1646 - mean_squared_error: 7.1646\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 178us/sample - loss: 7.0346 - mean_squared_error: 7.0346\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 227us/sample - loss: 6.9984 - mean_squared_error: 6.9984\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 0s 153us/sample - loss: 6.9954 - mean_squared_error: 6.9954\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 317us/sample - loss: 22.1290 - mean_squared_error: 22.1290\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 0s 148us/sample - loss: 7.1368 - mean_squared_error: 7.1368\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 0s 155us/sample - loss: 6.9427 - mean_squared_error: 6.9427\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 0s 163us/sample - loss: 6.8390 - mean_squared_error: 6.8390\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 0s 143us/sample - loss: 6.8036 - mean_squared_error: 6.8036\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 304us/sample - loss: 31.5250 - mean_squared_error: 31.5250\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 0s 159us/sample - loss: 7.9232 - mean_squared_error: 7.9232\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 0s 154us/sample - loss: 7.7425 - mean_squared_error: 7.7425\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 0s 148us/sample - loss: 7.5969 - mean_squared_error: 7.5969\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 1s 178us/sample - loss: 7.4383 - mean_squared_error: 7.4383\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 7s 2ms/sample - loss: 25.6351 - mean_squared_error: 25.6351\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 6s 2ms/sample - loss: 7.2708 - mean_squared_error: 7.2708\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 6s 2ms/sample - loss: 7.0312 - mean_squared_error: 7.0312\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 6s 2ms/sample - loss: 6.8909 - mean_squared_error: 6.8909\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 6s 2ms/sample - loss: 6.8145 - mean_squared_error: 6.8145\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 3s 964us/sample - loss: 17.2001 - mean_squared_error: 17.2001\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 2s 669us/sample - loss: 6.9284 - mean_squared_error: 6.9284\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 2s 794us/sample - loss: 6.7819 - mean_squared_error: 6.7819\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 3s 909us/sample - loss: 6.8461 - mean_squared_error: 6.8461\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 3s 1ms/sample - loss: 6.7187 - mean_squared_error: 6.7187\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 350us/sample - loss: 23.6463 - mean_squared_error: 23.6463\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 0s 165us/sample - loss: 6.6978 - mean_squared_error: 6.6978\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 171us/sample - loss: 6.6412 - mean_squared_error: 6.6412\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 0s 168us/sample - loss: 6.5600 - mean_squared_error: 6.5600\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 0s 162us/sample - loss: 6.4991 - mean_squared_error: 6.4991\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 371us/sample - loss: 29.8543 - mean_squared_error: 29.8543\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 1s 175us/sample - loss: 6.9619 - mean_squared_error: 6.9619\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 196us/sample - loss: 6.9764 - mean_squared_error: 6.9764\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 205us/sample - loss: 6.8023 - mean_squared_error: 6.8023\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 0s 155us/sample - loss: 6.6906 - mean_squared_error: 6.6906\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 42.7257 - mean_squared_error: 42.7258\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 9.1052 - mean_squared_error: 9.1052\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 8.6947 - mean_squared_error: 8.6947\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 8.4483 - mean_squared_error: 8.4483\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 3s 1ms/sample - loss: 8.2054 - mean_squared_error: 8.2054\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 2s 796us/sample - loss: 35.7025 - mean_squared_error: 35.7025\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 1s 445us/sample - loss: 6.8195 - mean_squared_error: 6.8195\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 2s 601us/sample - loss: 6.5024 - mean_squared_error: 6.5024\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 1s 248us/sample - loss: 6.4403 - mean_squared_error: 6.4403\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 2s 662us/sample - loss: 6.3945 - mean_squared_error: 6.3945\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 1s 493us/sample - loss: 13.4962 - mean_squared_error: 13.4962\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 2s 542us/sample - loss: 6.4754 - mean_squared_error: 6.4754\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.3453 - mean_squared_error: 6.3453\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.3009 - mean_squared_error: 6.3009\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.1953 - mean_squared_error: 6.1953\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 283us/sample - loss: 35.6063 - mean_squared_error: 35.6063\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 0s 128us/sample - loss: 6.7920 - mean_squared_error: 6.7920\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 168us/sample - loss: 6.5583 - mean_squared_error: 6.5583\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 200us/sample - loss: 6.4719 - mean_squared_error: 6.4719\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 0s 113us/sample - loss: 6.4933 - mean_squared_error: 6.4933\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 425us/sample - loss: 17.7106 - mean_squared_error: 17.7106\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 1s 276us/sample - loss: 6.6156 - mean_squared_error: 6.6156\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 225us/sample - loss: 6.4510 - mean_squared_error: 6.4510\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 205us/sample - loss: 6.5027 - mean_squared_error: 6.5027\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 1s 229us/sample - loss: 6.4589 - mean_squared_error: 6.4589\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 2s 591us/sample - loss: 15.0581 - mean_squared_error: 15.0581\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 1s 368us/sample - loss: 6.6443 - mean_squared_error: 6.6443\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 2s 537us/sample - loss: 6.4233 - mean_squared_error: 6.4233\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 292us/sample - loss: 6.3418 - mean_squared_error: 6.3418\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 1s 381us/sample - loss: 6.1740 - mean_squared_error: 6.1740\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 6s 2ms/sample - loss: 12.6024 - mean_squared_error: 12.6024\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 5s 2ms/sample - loss: 6.7097 - mean_squared_error: 6.7097\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 5s 2ms/sample - loss: 6.6026 - mean_squared_error: 6.6026\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 5s 2ms/sample - loss: 6.3857 - mean_squared_error: 6.3857\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 5s 2ms/sample - loss: 6.3836 - mean_squared_error: 6.3836\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 5s 2ms/sample - loss: 24.0080 - mean_squared_error: 24.0080\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.6629 - mean_squared_error: 6.6629\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.5938 - mean_squared_error: 6.5938\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.5909 - mean_squared_error: 6.5909\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.5339 - mean_squared_error: 6.5339\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 6s 2ms/sample - loss: 34.2859 - mean_squared_error: 34.2859\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 6s 2ms/sample - loss: 7.3057 - mean_squared_error: 7.3057\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 6s 2ms/sample - loss: 7.2780 - mean_squared_error: 7.2780\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 6s 2ms/sample - loss: 7.1749 - mean_squared_error: 7.1749\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 6s 2ms/sample - loss: 7.0939 - mean_squared_error: 7.0939\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 3s 913us/sample - loss: 31.5728 - mean_squared_error: 31.5728\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 2s 720us/sample - loss: 10.0774 - mean_squared_error: 10.0774\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 2s 713us/sample - loss: 9.2077 - mean_squared_error: 9.2077\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 2s 678us/sample - loss: 8.4770 - mean_squared_error: 8.4770\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 2s 780us/sample - loss: 8.4094 - mean_squared_error: 8.4094\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 368us/sample - loss: 26.4872 - mean_squared_error: 26.4872\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 0s 109us/sample - loss: 7.6767 - mean_squared_error: 7.6767\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 0s 126us/sample - loss: 7.1531 - mean_squared_error: 7.1531\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 0s 114us/sample - loss: 7.1122 - mean_squared_error: 7.1122\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 0s 119us/sample - loss: 7.0609 - mean_squared_error: 7.0609\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 1s 479us/sample - loss: 14.9318 - mean_squared_error: 14.9318\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 1s 323us/sample - loss: 6.9541 - mean_squared_error: 6.9541\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 1s 305us/sample - loss: 6.9206 - mean_squared_error: 6.9206\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 1s 317us/sample - loss: 6.9400 - mean_squared_error: 6.9400\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 1s 308us/sample - loss: 6.9599 - mean_squared_error: 6.9599\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 3s 994us/sample - loss: 19.4193 - mean_squared_error: 19.4193\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 5s 2ms/sample - loss: 6.5627 - mean_squared_error: 6.5627\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 2s 526us/sample - loss: 6.7037 - mean_squared_error: 6.7037\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 3s 952us/sample - loss: 6.5416 - mean_squared_error: 6.5416\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 6.4089 - mean_squared_error: 6.4089\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 3s 954us/sample - loss: 47.4945 - mean_squared_error: 47.4945\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 2s 804us/sample - loss: 7.2865 - mean_squared_error: 7.2865\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 2s 690us/sample - loss: 6.9040 - mean_squared_error: 6.9040\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 2s 700us/sample - loss: 6.6737 - mean_squared_error: 6.6737\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 2s 772us/sample - loss: 6.6426 - mean_squared_error: 6.6426\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 2s 591us/sample - loss: 23.4006 - mean_squared_error: 23.4006\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 1s 385us/sample - loss: 7.3342 - mean_squared_error: 7.3342\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 255us/sample - loss: 7.3490 - mean_squared_error: 7.3490\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 222us/sample - loss: 7.1023 - mean_squared_error: 7.1023\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 1s 217us/sample - loss: 7.1630 - mean_squared_error: 7.1630\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 382us/sample - loss: 31.0696 - mean_squared_error: 31.0695\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 0s 133us/sample - loss: 10.1882 - mean_squared_error: 10.1882\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 0s 149us/sample - loss: 9.2765 - mean_squared_error: 9.2765\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 0s 166us/sample - loss: 8.8638 - mean_squared_error: 8.8638\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 0s 143us/sample - loss: 8.4841 - mean_squared_error: 8.4841\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 2s 795us/sample - loss: 63.3754 - mean_squared_error: 63.3754\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 1s 499us/sample - loss: 9.2622 - mean_squared_error: 9.2622\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 2s 670us/sample - loss: 8.3088 - mean_squared_error: 8.3088\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 2s 667us/sample - loss: 8.0146 - mean_squared_error: 8.0146\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 2s 685us/sample - loss: 7.6240 - mean_squared_error: 7.6240\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 1s 361us/sample - loss: 24.4183 - mean_squared_error: 24.4184\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 1s 274us/sample - loss: 6.5806 - mean_squared_error: 6.5806\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 1s 233us/sample - loss: 6.4406 - mean_squared_error: 6.4406\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 1s 225us/sample - loss: 6.3848 - mean_squared_error: 6.3848\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 1s 402us/sample - loss: 6.3403 - mean_squared_error: 6.3403\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 307us/sample - loss: 19.8327 - mean_squared_error: 19.8327\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 0s 155us/sample - loss: 7.3386 - mean_squared_error: 7.3386\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 0s 156us/sample - loss: 7.1874 - mean_squared_error: 7.1874\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 0s 142us/sample - loss: 6.9192 - mean_squared_error: 6.9192\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 0s 144us/sample - loss: 6.7069 - mean_squared_error: 6.7069\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 15.0333 - mean_squared_error: 15.0333\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 3s 911us/sample - loss: 6.5141 - mean_squared_error: 6.5141\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 5s 2ms/sample - loss: 6.5429 - mean_squared_error: 6.5429\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 6.2443 - mean_squared_error: 6.2443\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 4s 1ms/sample - loss: 6.1741 - mean_squared_error: 6.1741\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 401us/sample - loss: 30.8780 - mean_squared_error: 30.8780\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 1s 224us/sample - loss: 7.5026 - mean_squared_error: 7.5026\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 247us/sample - loss: 7.5904 - mean_squared_error: 7.5904\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 252us/sample - loss: 7.4738 - mean_squared_error: 7.4738\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 1s 247us/sample - loss: 7.3516 - mean_squared_error: 7.3516\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 5s 2ms/sample - loss: 24.3261 - mean_squared_error: 24.3261\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 7.6830 - mean_squared_error: 7.6830\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 7.6106 - mean_squared_error: 7.6106\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 7.4384 - mean_squared_error: 7.4384\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 4s 1ms/sample - loss: 7.3313 - mean_squared_error: 7.3313\n",
      "Train on 2969 samples\n",
      "Epoch 1/5\n",
      "2969/2969 [==============================] - 1s 268us/sample - loss: 28.8196 - mean_squared_error: 28.8196\n",
      "Epoch 2/5\n",
      "2969/2969 [==============================] - 0s 107us/sample - loss: 6.7736 - mean_squared_error: 6.7736\n",
      "Epoch 3/5\n",
      "2969/2969 [==============================] - 0s 97us/sample - loss: 6.7621 - mean_squared_error: 6.7621\n",
      "Epoch 4/5\n",
      "2969/2969 [==============================] - 0s 133us/sample - loss: 6.7795 - mean_squared_error: 6.7795\n",
      "Epoch 5/5\n",
      "2969/2969 [==============================] - 0s 115us/sample - loss: 6.8206 - mean_squared_error: 6.8206\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 461us/sample - loss: 19.5157 - mean_squared_error: 19.5157\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 1s 318us/sample - loss: 6.5368 - mean_squared_error: 6.5368\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 306us/sample - loss: 6.3092 - mean_squared_error: 6.3092\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 261us/sample - loss: 6.2403 - mean_squared_error: 6.2403\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 1s 343us/sample - loss: 6.2503 - mean_squared_error: 6.2503\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 441us/sample - loss: 14.6886 - mean_squared_error: 14.6886\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 1s 246us/sample - loss: 6.6878 - mean_squared_error: 6.6878\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 1s 210us/sample - loss: 6.5130 - mean_squared_error: 6.5130\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 1s 210us/sample - loss: 6.3791 - mean_squared_error: 6.3791\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 1s 234us/sample - loss: 6.4315 - mean_squared_error: 6.4315\n",
      "Train on 2970 samples\n",
      "Epoch 1/5\n",
      "2970/2970 [==============================] - 1s 260us/sample - loss: 49.3218 - mean_squared_error: 49.3218\n",
      "Epoch 2/5\n",
      "2970/2970 [==============================] - 0s 111us/sample - loss: 8.3800 - mean_squared_error: 8.3800\n",
      "Epoch 3/5\n",
      "2970/2970 [==============================] - 0s 94us/sample - loss: 7.7259 - mean_squared_error: 7.7259\n",
      "Epoch 4/5\n",
      "2970/2970 [==============================] - 0s 108us/sample - loss: 7.5998 - mean_squared_error: 7.5998\n",
      "Epoch 5/5\n",
      "2970/2970 [==============================] - 0s 99us/sample - loss: 7.4577 - mean_squared_error: 7.4577\n",
      "time : 538.7116413116455\n"
     ]
    }
   ],
   "source": [
    "# 5.\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "total_val = 10000\n",
    "trial = 100\n",
    "kfold = kf(n_splits = 5)\n",
    "oplist = []\n",
    "for _ in range(trial):\n",
    "    instant_val = 0\n",
    "    for train_index, test_index in kfold.split(Xtot):\n",
    "        X_train, X_test = Xtot[train_index], Xtot[test_index]\n",
    "        y_train, y_test = ytot[train_index], ytot[test_index]\n",
    "        \n",
    "        c1n = random.choice(hyperdict['con1_num'])\n",
    "        c1l = random.choice(hyperdict['con1_len'])\n",
    "        c2p = random.choice(hyperdict['con2_prob'])\n",
    "        c2n = random.choice(hyperdict['con2_num'])\n",
    "        c2l = random.choice(hyperdict['con2_len'])\n",
    "        denp =random.choice(hyperdict['den_prob'])\n",
    "        den1 = random.choice(hyperdict['den1_len'])\n",
    "        dout = random.choice(hyperdict['droprate'])\n",
    "        ep = random.choice(hyperdict['epochs'])\n",
    "        b_size = random.choice(hyperdict['batch_size'])\n",
    "        model = create_model(c1n,c1l,c2p,c2n,c2l,denp,den1,dout)\n",
    "        model.compile(optimizer = 'Adam', loss = 'mean_squared_error', metrics = ['mean_squared_error'])\n",
    "        history = model.fit(X_train, y_train, epochs = ep, verbose = 1, batch_size = b_size)    \n",
    "        instant_val += history.history['loss'][-1]\n",
    "    if total_val >instant_val/5:\n",
    "        model.save('CNN_model.h5')\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstonek",
   "language": "python",
   "name": "capstonek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
