





# 1.

from __future__ import absolute_import, division, print_function, unicode_literals
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import numpy as np
import random
import time
#import kFold
import itertools
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold as kf
import theano
from keras import optimizers
from tensorflow.keras import datasets, layers, models
from scipy.stats import pearsonr
from sklearn.model_selection import KFold
from scipy.stats import pearsonr
from tqdm import tqdm
data = pd.read_csv('PCC6803 Promoter and reads 100bp.csv')


data.head()





# 2.

def one_hot_encoding(df, seq_column, expression):
    bases = ['A','C','G','T']
    base_dict = dict(zip(bases,range(4)))
    n = len(df)
    total_width = df[seq_column].str.len().max()+20
    X = np.zeros((n,1,4,total_width))
    seqs = df[seq_column].values
    for i in range(n):
        seq = seqs[i]
        for b in range(len(seq)):
            X[i,0,base_dict[seq[b]], b+10+100-len(seq)] = 1    
    X = X.astype(theano.config.floatX)
    y = np.asarray(df[expression].values, dtype = theano.config.floatX)[:,np.newaxis]
    return X, y, total_width

Xtot, ytot, _ = one_hot_encoding(data,'Promoter','Reads')
ytot= np.log2(ytot+1)
print(Xtot.shape)
print(Xtot)
print(ytot)





# 3.

hyperdict = {'con1_num' : [4,8,16,32],'con1_len' : [6,12,18,24], 'con2_prob' : [0,1],
             'con2_num' : [4,8,16,32],'con2_len' : [6,12,18,24],'den_prob' : [0,1],
             'den1_len' : [4,8,16,32], 'droprate' : [0.1,0.2,0.3,0.5] , 'batch_size' :[32,64], 'epochs' : [50, 75,100, 150, 200]} 





# 4.

def create_model(con1n = None, con1l= None,con2p = None, con2n= None, con2l= None,denp = None,den1= None, dout = None):
    model= models.Sequential()
    model.add(layers.Conv2D(con1n,(4, con1l), activation = 'relu', data_format = 'channels_first', input_shape = (1,4,120)))
    model.add(layers.Dropout(dout))
    if con2p == 1:     
        model.add(layers.Conv2D(con2n,(1,con2l),activation = 'relu' ,data_format = 'channels_first'))
        model.add(layers.Dropout(dout))
    model.add(layers.Flatten())
    model.add(layers.Dropout(dout))
    if denp == 1:
        model.add(layers.Dense(den1, activation = 'relu'))
    model.add(layers.Dense(1))
    return model





# 5.

start = time.time() 

total_val = 10000
trial = 100
kfold = kf(n_splits = 5)
oplist = []
for _ in range(trial):
    instant_val = 0
    for train_index, test_index in kfold.split(Xtot):
        X_train, X_test = Xtot[train_index], Xtot[test_index]
        y_train, y_test = ytot[train_index], ytot[test_index]
        
        c1n = random.choice(hyperdict['con1_num'])
        c1l = random.choice(hyperdict['con1_len'])
        c2p = random.choice(hyperdict['con2_prob'])
        c2n = random.choice(hyperdict['con2_num'])
        c2l = random.choice(hyperdict['con2_len'])
        denp =random.choice(hyperdict['den_prob'])
        den1 = random.choice(hyperdict['den1_len'])
        dout = random.choice(hyperdict['droprate'])
        ep = random.choice(hyperdict['epochs'])
        b_size = random.choice(hyperdict['batch_size'])
        model = create_model(c1n,c1l,c2p,c2n,c2l,denp,den1,dout)
        model.compile(optimizer = 'Adam', loss = 'mean_squared_error', metrics = ['mean_squared_error'])
        history = model.fit(X_train, y_train, epochs = ep, verbose = 1, batch_size = b_size)    
        instant_val += history.history['loss'][-1]
    if total_val >instant_val/5:
        model.save('CNN_model.h5')
print("time :", time.time() - start)


import time
import random
from sklearn.model_selection import KFold
from scipy.stats import pearsonr
import numpy as np
import matplotlib.pyplot as plt

# Assuming Xtot and ytot are your features and labels respectively.
# Also assuming hyperdict and create_model are already defined.

start = time.time()

total_val = 10000
trial = 100
kfold = KFold(n_splits=5)
oplist = []

best_correlation = -1  # Initialize best correlation
best_model = None  # Initialize best model
best_hyperparameters = {}  # To store the best hyperparameters

for _ in range(trial):
    instant_val = 0
    correlations = []
    for train_index, test_index in kfold.split(Xtot):
        X_train, X_test = Xtot[train_index], Xtot[test_index]
        y_train, y_test = ytot[train_index], ytot[test_index]

        c1n = random.choice(hyperdict['con1_num'])
        c1l = random.choice(hyperdict['con1_len'])
        c2p = random.choice(hyperdict['con2_prob'])
        c2n = random.choice(hyperdict['con2_num'])
        c2l = random.choice(hyperdict['con2_len'])
        denp = random.choice(hyperdict['den_prob'])
        den1 = random.choice(hyperdict['den1_len'])
        dout = random.choice(hyperdict['droprate'])
        ep = random.choice(hyperdict['epochs'])
        b_size = random.choice(hyperdict['batch_size'])
        
        model = create_model(c1n, c1l, c2p, c2n, c2l, denp, den1, dout)
        model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['mean_squared_error'])
        history = model.fit(X_train, y_train, epochs=ep, verbose=1, batch_size=b_size, validation_data=(X_test, y_test))
        
        # Make predictions on the test set
        y_pred = model.predict(X_test)
        
        # Calculate the correlation between predicted and actual values
        correlation, _ = pearsonr(y_test.flatten(), y_pred.flatten())
        correlations.append(correlation)
        
        # Accumulate the validation loss
        instant_val += history.history['val_loss'][-1]

    # Average correlation across the folds
    avg_correlation = np.mean(correlations)
    
    if avg_correlation > best_correlation:
        best_correlation = avg_correlation
        best_model = model
        best_hyperparameters = {
            'c1n': c1n,
            'c1l': c1l,
            'c2p': c2p,
            'c2n': c2n,
            'c2l': c2l,
            'denp': denp,
            'den1': den1,
            'dout': dout,
            'ep': ep,
            'b_size': b_size
        }
        best_model.save('CNN_model.h5')

print(f"Best correlation: {best_correlation}")
print(f"Best hyperparameters: {best_hyperparameters}")
print("Time elapsed:", time.time() - start)

# Retrain the best model on the entire dataset
best_model = create_model(
    best_hyperparameters['c1n'],
    best_hyperparameters['c1l'],
    best_hyperparameters['c2p'],
    best_hyperparameters['c2n'],
    best_hyperparameters['c2l'],
    best_hyperparameters['denp'],
    best_hyperparameters['den1'],
    best_hyperparameters['dout']
)
best_model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['mean_squared_error'])
best_model.fit(Xtot, ytot, epochs=best_hyperparameters['ep'], verbose=1, batch_size=best_hyperparameters['b_size'])

# Make predictions on the entire dataset
y_pred_tot = best_model.predict(Xtot)

# Plot the correlation between experimental results and predicted values
plt.figure(figsize=(10, 6))
plt.scatter(ytot.flatten(), y_pred_tot.flatten(), alpha=0.5)
plt.plot([min(ytot.flatten()), max(ytot.flatten())], [min(ytot.flatten()), max(ytot.flatten())], color='red', linestyle='--')
plt.xlabel('Experimental Results')
plt.ylabel('Predicted Values')
plt.title('Correlation Plot: Experimental vs. Predicted Values')
plt.grid(True)
plt.show()

